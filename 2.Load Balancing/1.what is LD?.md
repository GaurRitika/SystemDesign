##  What is Load Balancing?

**Load Balancing** is the technique of **distributing incoming network traffic or workload across multiple servers** so that no single server is overwhelmed — ensuring **high availability, reliability, and performance**.

---

###  Real-life Analogy

Imagine you own a restaurant chain �️

* You have 5 branches (servers).
* Customers (users) enter from one main reception desk (load balancer).
* The receptionist decides which branch should handle the next customer — based on who’s least busy.

➡️ That receptionist = **Load Balancer**
➡️ Branches = **Servers**
➡️ Customers = **Client requests**

This ensures all branches work efficiently and no single one gets overloaded.

---

##  Why Load Balancing?

When you have **millions of users (e.g., Instagram, YouTube)**, one server can’t handle all requests.
Without balancing:

* One server may crash under heavy traffic.
* Others may remain idle.

So, **load balancers**:

* Split requests efficiently across servers.
* Keep the system available even if one server fails.

---

##  Types of Load Balancing

There are **two broad types**:

---

### **1. Hardware Load Balancers**

* Physical devices (e.g., F5, Citrix NetScaler)
* Installed in data centers
* Used by large enterprises
* Super fast, but expensive and less flexible

---

### **2. Software Load Balancers**

* Run as programs or services
* Examples: **Nginx**, **HAProxy**, **Envoy**, **AWS Elastic Load Balancer**
* Cheaper, more flexible, easily scalable
* Commonly used in cloud architectures

---

##  Load Balancer Architecture (Simplified)

```
        ┌───────────────┐
        │   Clients     │
        │ (Users, Apps) │
        └──────┬────────┘
               │
               ▼
      ┌──────────────────┐
      │  Load Balancer   │
      │ (Decides Target) │
      └──────┬───────────┘
             │
 ┌───────────┼────────────┐
 ▼           ▼            ▼
Server A   Server B     Server C
```

**Process:**

1. Clients send requests to one IP (load balancer IP).
2. Load balancer chooses a backend server using an **algorithm**.
3. Response is sent back to client transparently.

---

##  Load Balancing Algorithms

Different ways to decide *which server gets the next request* 👇

| Algorithm                          | Description                                                   | Use Case                                  |
| ---------------------------------- | ------------------------------------------------------------- | ----------------------------------------- |
| **Round Robin**                    | Sends each request to the next server in order                | Simple, good when servers are similar     |
| **Weighted Round Robin**           | Some servers get more requests based on power/capacity        | When servers have different specs         |
| **Least Connections**              | Send new request to the server with fewest active connections | Dynamic loads (e.g., DB-heavy apps)       |
| **IP Hash**                        | Client’s IP decides which server to send to                   | Sticky sessions (same user → same server) |
| **Random**                         | Pick any server randomly                                      | Testing or small scale                    |
| **Response Time / CPU load-based** | Send to server with fastest response or lowest CPU            | Intelligent, adaptive balancing           |

---

##  Example — Instagram Feed System

When you open Instagram:

1. Request → `loadbalancer.instagram.com`
2. Load Balancer checks backend health (availability)
3. Applies balancing rule (e.g., least connections)
4. Sends request to `feed-server-3`
5. Server processes and sends feed data → Load balancer → You

If `feed-server-3` crashes, LB automatically reroutes traffic to another server.

---

##  Benefits of Load Balancing

| Benefit               | Explanation                                    |
| --------------------- | ---------------------------------------------- |
| **Scalability**       | Easily add more servers under the balancer     |
| **High Availability** | If one server fails, others handle load        |
| **Fault Tolerance**   | Auto rerouting keeps system up                 |
| **Performance**       | Distributes load evenly → faster responses     |
| **Security**          | LB hides internal servers behind one public IP |

---

##  Load Balancing Strategies (System Design View)

| Layer                      | Type                              | Example                   |
| -------------------------- | --------------------------------- | ------------------------- |
| **DNS Level**              | Uses multiple IPs for a domain    | Cloudflare, AWS Route53   |
| **Transport Level (L4)**   | Uses IP & Port info (no app data) | HAProxy, Nginx (TCP mode) |
| **Application Level (L7)** | Uses HTTP headers, cookies, URLs  | Nginx, Envoy, AWS ALB     |

---

##  Health Checks

Load balancers **periodically ping servers** to see if they’re alive:

* Ping `/health` or `/status` endpoint
* If no response → temporarily remove server from pool
* Auto-recover when server comes back online

This ensures **fault tolerance** and **zero downtime**.

---

##  Session Persistence (Sticky Sessions)

Sometimes, you want **the same user to always go to the same server**, especially if the session is stored in memory.

Example:

* User logs into Instagram.
* Load balancer routes them to Server B.
* Server B stores session in memory.
* LB ensures same user → same server next time (using IP hash or cookies).

---

##  Example: System Design Diagram (Instagram)

```
            ┌───────────────┐
            │   Users       │
            └──────┬────────┘
                   │
                   ▼
        ┌────────────────────┐
        │  Load Balancer     │
        │ (Nginx / AWS ALB)  │
        └──────┬─────────────┘
               │
 ┌─────────────┼────────────────┐
 ▼             ▼                ▼
App Server 1  App Server 2    App Server 3
   |             |                |
   ▼             ▼                ▼
Database       Cache            Media Server
```

---

##  Interview-Level Answer:

> **Load Balancing** is the process of distributing incoming traffic or computational load across multiple servers to ensure no single server becomes a bottleneck. It improves performance, availability, and fault tolerance. Load balancers can use algorithms like Round Robin, Least Connections, or IP Hash, and can operate at various layers (L4 or L7).

---

##  Real Systems Using Load Balancing

| Company    | Load Balancer Used          |
| ---------- | --------------------------- |
| Instagram  | Nginx + Envoy               |
| Netflix    | Zuul + AWS ALB              |
| Google     | Maglev (custom LB)          |
| Amazon     | Elastic Load Balancer (ELB) |
| Cloudflare | Global Anycast LB           |

---
